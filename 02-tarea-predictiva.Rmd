# La tarea predictiva fundamental

```{r, include = FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 13))
cbb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = cbb_palette)
}
```

En esta parte discutiremos en qué consiste el aprendizaje supervisado, 
cómo medimos el desempeño de nuestras predicciones, y cómo entender
este desempeño en el contexto del problema que queremos resolver.

```{block2, type="resumen"}
El objetivo principal en el aprendizaje supervisado es

- Usar **datos etiquetados** para construir **modelos**
- Usar estos modelos para hacer predicciones precisas de **nuevos casos** 

```

En función de esto, definimos la siguiente notación. Tenemos *datos de entrenamiento*
de la forma
$$(x^{(i)}, y^{(i)}) = \left ( (x_1^{(i)}, x_2^{(i)}, \ldots, x_p^{(i)}), y^{(i)} \right)$$
a $x_1, x_2, \ldots, x_p$ les llamamos *variables de entrada*, y
$y$ es la *respuesta*. El **conjunto de entrenamiento** es

$${\mathcal L} =  (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(N)}, y^{(N)}) $$
Usando estos datos, buscamos construir una función

$${\mathcal L} \to \hat{f} = f_{\mathcal L}$$. 

Si observamos en el futuro un nuevo caso con
variables de entrada $\mathbf{x} = (\mathbf{x}_1, \ldots, \mathbf{x}_p)$, nuestra predicción sería

$$\hat{\mathbf{y}} = \hat{f} (\mathbf{x}),$$

y una vez que conocemos el verdadero valor $\mathbf{y}$ de la variable respuesta, quisiéramos
que nuestra predicción $\hat{\mathbf{y}}$ esté *cercana* al verdadero valor $\mathbf{y}$. La
definición de *cercana* puede depender del problema particular. 

Típicamente nos interesa hacer más de una predicción individual, y evaluar
el desempeño en una población dada para la cual no conocemos la respuesta.
Así que quisiéramos evaluar varios casos nuevos, que de preferencia son una muestra
grande
del universo de datos para los que nos interesa hacer predicciones. Para
esto necesitamos un **conjunto de datos de prueba** suficientemente grande y representativo, 
que denotamos por:

$${\mathcal T} = (\mathbf{x}^{(1)}, \mathbf{y}^{(1)}), (\mathbf{x}^{(2)}, \mathbf{y}^{(2)}), \ldots, (\mathbf{x}^{(m)}, \mathbf{y}^{(m)}),$$


Al principio no conocemos la respuesta $\mathbf{y}^{(i)}$ así que hacemos las predicciones
$$\hat{\mathbf{y}}^{(i)} = \hat{f}(\mathbf{x}^{(i)}).$$

Finalmente, una vez que conocemos los valores
de la respuesta verdaderos, 
medimos el desempeño de nuestro modelo comparando $\hat{\mathbf{y}}^{(i)}$ con
${\mathbf{y}}^{(i)}$. Si en general estos valores están cercanos, entonces consideramos
que nuestras predicciones son buenas.

```{block2, type="resumen"}
- Con el conjunto de datos de entrenamiento construimos nuestra función de predicción $\hat{f}$.
- Con el conjunto de datos de prueba evaluamos el desempeño predictivo de nuestro modelo.
- El conjunto de datos de prueba **no** debe ser utilizado en la construcción de la función
de predicción $\hat{f}$
- No tiene sentido usar los datos de prueba para construir la función de predictor: los
casos de prueba son análogos a las *preguntas de un examen*. Los datos de entrenamiento
son los casos que "mostramos" al modelo para aprender a contestar esas preguntas
```

## Medidas de error de predicción

Hay varias maneras de medir el error de cada predicción particular. Es común, por ejemplo,
usar el error cuadrático
$$L({\mathbf{y}}^{(i)}, \hat{\mathbf{y}}^{(i)}) = \left ( {\mathbf{y}}^{(i)} - \hat{\mathbf{y}}^{(i)} \right )^2$$
o también el error absoluto
$$L({\mathbf{y}}^{(i)}, \hat{\mathbf{y}}^{(i)}) = \left | {\mathbf{y}}^{(i)} - \hat{\mathbf{y}}^{(i)} \right |$$
Y usualmente definimos el **error de prueba** $\hat{Err}$ como el promedio de error sobre los datos
de prueba. Por ejemplo, para el error absoluto calcularíamos:

$$\hat{Err} = \frac{1}{m}\sum_i  \left | {\mathbf{y}}^{(i)} - \hat{\mathbf{y}}^{(i)} \right |$$

Este tipo de medidas promedio es adecuado cuando hacemos muchas predicciones, y tiene
sentido usar el promedio como medida general del desempeño predictivo. Cuando sólo queremos
hacer unas cuantas predicciones importantes, típicamente es necesario hacer una cuantificación más
detallada de lo que puede suceder para distintas predicciones (por ejemplo usando intervalos
de confianza o probabilidad, como veremos más adelante).



## Ejemplo: un ajuste lineal

Consideremos un ejemplo simple con datos simulados. Sólo hay una variable
de entrada (lo cual es muy poco común), y supongamos que los datos que hemos
observado hasta ahora son los siguientes:

```{r, fig.width=5, fig.asp=0.7}
set.seed(1424)
simular_ejemplo <- function(n, sd = 500){
  x <- runif(n, 0, 20)
  y <- ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
  y <- y + rnorm(n, 0, sd = sd)
  tibble(id = 1:n, x = x, y = y)
}
datos_entrena <- simular_ejemplo(30)
datos_prueba <- simular_ejemplo(1000)
datos_x_prueba <- datos_prueba %>% select(-y)
ggplot(datos_entrena, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Nótese que agregamos una recta de mínimos cuadrados con la que pretendemos 
hacer predicciones. Podemos construirla de la siguiente forma:

```{r, message=FALSE, warning=FALSE}
library(tidymodels)
# metricas de evaluacion
# definir metricas
mis_metricas <- metric_set(mae)
# receta de preprocesamiento y definición de papeles de variables
receta_simple <- recipe(y ~ ., data = datos_entrena) %>%
  update_role(id, new_role = "id")
# modelo
modelo <- linear_reg() %>% set_engine("lm")
#modelo <- linear_reg() %>% set_engine("keras")
# flujo para el pipeline de procesamiento y ajuste
flujo_simple <- workflow() %>% 
  add_recipe(receta_simple) %>% 
  add_model(modelo)
```

Ahora ajustamos:

```{r}
ajuste <- fit(flujo_simple, datos_entrena)
pull_workflow_fit(ajuste)
```

Una vez que hemos terminado, podemos hacer predicciones para
nuestros datos de prueba:

```{r}
preds <- predict(ajuste, datos_x_prueba)
preds
```
Finalmente, revelamos los valores de respuesta $y$ observados,
y evaluamos usando el error absoluto promedio sobre la muestra de prueba:

```{r}
datos_eval <- preds %>% 
  bind_cols(datos_prueba)
eval_lineal <- mis_metricas(datos_eval, truth = y, estimate = .pred) %>% 
  mutate(modelo = "lineal") %>% select(modelo, everything())
eval_lineal
```

## Ejemplo: vecinos cercanos

Veamos otra manera de hacer predicciones. En este ejemplo usamos la técnica
de 1-vecino más cercano, que consiste en predecir para una $\mathbf{x}$ buscando
entre los casos de entrenamiento aquel cuya $x^{(i)}$ esté más cercana a
$\mathbf{x}$, y tomando el valor correspondiente de $y$:

```{r}
modelo_vecino <- nearest_neighbor(neighbors = 1) %>% 
  set_mode("regression") %>% 
  set_engine("kknn")
flujo_simple <- workflow() %>% 
  add_recipe(receta_simple) %>% 
  add_model(modelo_vecino)
ajuste <- fit(flujo_simple, datos_entrena)
```

El error de prueba con una muestra independiente de la de entrenamiento es:

```{r}
preds <- predict(ajuste, datos_x_prueba)
datos_eval <- preds %>% 
  bind_cols(datos_prueba)
eval_lineal <- mis_metricas(datos_eval, truth = y, estimate = .pred) %>% 
  mutate(modelo = "1-vecino") %>% select(modelo, everything())
eval_lineal
```

En este par de ejemplos, es método lineal parece funcionar mejor, pues
el error de prueba con una muestra grande es más bajo con el modelo lineal
con el de 1 vecino más cercano. 

```{r}
graf_x <- tibble(x = seq(0, 20, 0.1), id = NA)
preds_x <- predict(ajuste, graf_x)
datos_graf <- preds_x %>% bind_cols(graf_x)
ggplot(datos_entrena, aes(x = x)) +
  geom_line(data= datos_graf, aes(y = .pred), colour = "red") +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y), method = "lm", se = FALSE)
```


**Nota que el error de este ajuste sobre los datos de entrenamiento es cero. Sin embargo,
mostramos arriba que el error de prueba sobre una muestra nueva es considerablemente
más alta.**

```{block2, type="resumen"}
**Error de prueba y error de entrenamiento**

- Usamos el **error de prueba** $\hat{Err}$ para evaluar el desempeño predictivo de los modelos,
que se calcula sobre una muestra diferente a la que se usó para entrenar nuestro modelo

$$\hat{Err} = \frac{1}{m}\sum_i  L \left ({\mathbf{y}}^{(i)} , \hat{\mathbf{y}}^{(i)} \right)$$


- El **error de entrenamiento** es una cantidad que usamos más para diagnosticar que
para evaluar, y está dada por


$$\overline{err} = \frac{1}{N}\sum_{i=1}^N L \left (y^{(i)} , \hat{f}(x^{(i)}) \right )$$
```


Usualmente $\overline{err} < \hat{Err}$: siempre es más fácil para un modelo predecir datos
que ha visto que datos nuevos de la muestra de prueba. 

Obsérvese que si el conjunto de prueba es una muestra iid, entonces el error
de prueba estima el valor esperado del error cuando hacemos una predicción, que podemos
denotar como $Err$. Si la muestra de prueba es grande, entonces $\hat{Err} \approx Err$, usando
la ley de los grandes números. Pero ser muy complicado calcular el valor esperado de $\overline{err}$,
pues $\hat{f}$ depende de las $(x^{(i)}, y^{(i)})$.

## La tarea del análisis predictivo 

En resumen, la tarea del análisis predictivo es la siguiente: dados 

- Datos de entrenamiento $\mathcal{L}  = \{({x}^{(1)}, y^{(1)}), ({x}^{(2)}, y^{(2)}), \ldots, ({x}^{(n)}, y^{(n)})\}$
- Datos de prueba $\mathcal{T} =  \{(\mathbf{x}^{(1)}, \mathbf{y}^{(1)}), (\mathbf{x}^{(2)}, \mathbf{y}^{(2)}), \ldots, (\mathbf{x}^{(m)}, \mathbf{y}^{m)})\}$

Queremos construir una función $\mathcal{L} \to f$ (sólo depende de datos de entrenamiento) tal que el promedio
de error sobre los datos de prueba:

$$\hat{Err} = \frac{1}{m} \sum_{i = 1}^m L \left(\mathbf{y}^{(j)}, {f}(\mathbf{x}^{(j)})\right )$$

sea lo más chico posible. A esta cantidad le llamamos **error de prueba**.

Es crucial que los datos de entrenamiento sean distintos de los datos de prueba: esta es la
esencia de *aprender*, y no memorizar. Memorizar puede
producir sobreinterpretación de patrones que no generalizan en datos todavía no
vistos. Otra manera de decir esto es que buscamos buen desempeño **fuera de la muestra** con la que construimos el modelo. 

Esta formulación de la tarea del análisis predictivo ha sido muy productiva (@donoho50), y ha permitido
avances grandes en muchos problemas interesantes desde hace unas cuantas décadas. En parte
es porque 
 esta formulación es relativamente fácil de implementar
para agregar contribuciones de muchas personas. Sitios como [Kaggle](https://www.kaggle.com/) son implementaciones donde:

- Hay un conjunto de datos disponibles, con los que cualquiera puede construir modelos
- Existe un criterio fijo para medir error en las predicciones.
- Existen competidores que se inscriben y producen reglas de predicción suando los datos
disponibles.
- Hay un referee que evalúa las reglas de los concursantes *usando datos a los que sólo el referee
tiene acceso*.
- (En algunos casos) Al final del concurso, los competidores muestran su metodología.

**Ejemplo**: el concurso de Netflix (2009) siguió este patrón. De este concurso se desarrolló 
parcialmente el área ahora floreciente de sistemas de recomendación.

**Ejercicio**: Vista el sitio de Kaggle y examina algunos concursos: ¿cuáles son los datos
de entrenamiento y cuáles son los de prueba? ¿qué medida de error es la que se utiliza?


## El problema predictivo en contexto

Los elementos que hemos mostrado arriba proveen los fundamentos para el flujo
de trabajo en machine learning. Sin embargo, debemos tomar en cuenta que hay elementos
adicionales que hay que tomar en cuenta al resolver problemas prácticos o tomar
decisiones para problemas de negocios, política pública, etc. Todos estos problemas
tienen un contexto que no se puede ignorar.

En primer lugar, este contexto tienen qué ver con los costos y beneficios 
particulares de los errores en los
que incurrimos al tomar decisiones basadas en modelos. Estos costos son a veces 
difíciles de elicitar y cuantificar con precisión, pero en el análisis deben tomarse en cuenta
de alguna forma.

Supongamos por ejemplo que tenemos un plan o *tratamiento* para aumentar las compras de clientes
de una tienda en línea. Es un tratamiento relativamente costoso que no quisiéramos aplicar
a todos los clientes, sino que quisiéramos focalizarlo a aquellos clientes que tienen
riesgo de tener muy pocas compras, lo cual degrada su valor en nuestra cartera. Por ejemplo,
podríamos tener que:

- El tratamiento de retención cuesta 500 pesos por cliente,
- Estimamos mediante pruebas que nuestro tratamiento aumenta en 2000 pesos de un cliente
que gasta menos de 700 pesos, pero no aumenta las ventas si gasta más de 700 pesos.

Una pieza de este problema es entonces un modelo predictivo de las compras de un cliente para
el próximo mes en términos de su comportamiento pasado. Tenemos entonces un modelo
predictivo $\hat{f}(x)$ para las compras del próximo mes de cada cliente. Este modelo
tiene un error promedio de 25%. En primer lugar, después de aplicar nuestro
modelo y obtener las predicciones obtenemos:

```{r}
clientes <- tibble(id = 1:6000) %>% 
    mutate(pred = 5000*rbeta(length(id), 2, 3)) 
```

La pregunta es ¿a qué clientes nos conviene tratar? Pensemos que
queremos poner un punto de corte para las predicciones, de forma que si la predicción
es más baja que cierto punto de corte, entonces aplicamos el tratamiento.

Tenemos que hacer un
análisis **costo-beneficio**. Primero calculamos los costos:

```{r}
calc_costos <- function(corte, mejora, corte_trata, costo_trata){
    # compras de los que recibieron tratamiento
    compras_tratados <- filter(clientes, pred < corte) %>% 
        mutate(compras_sim = pred * (1 + rnorm(n(), 0, 0.25))) %>%
        mutate(compras_trata = ifelse(compras_sim < corte_trata, compras_sim + mejora, compras_sim)) %>% 
        summarise(total = sum(compras_trata), total_cf = sum(compras_sim))
    compras_trata <- pull(compras_tratados, total)
    compras_cf <- pull(compras_tratados, total_cf)
    # compras de los que no recibieron tratamiento
    compras_no_tratados <- filter(clientes, pred > corte) %>% 
        mutate(compras = pred * (1 + rnorm(n(), 0, 0.25))) %>%
        summarise(total = sum(compras)) %>% 
        pull(total)
    total <- compras_trata -  costo_trata*nrow(filter(clientes, pred < corte)) - compras_cf 
    total
}
perdidas_sim <- map_dfr(rep(seq(0 , 3000, 100), 100), 
    function(x){
      compras_sim <- calc_costos(x, mejora = 2500, corte_trata = 700, costo_trata = 500)
      tibble(compras = compras_sim, corte = x)
    }) %>% bind_rows 
```

```{r}
ggplot(perdidas_sim, aes(x = corte, y = compras / 1000)) +
  geom_jitter(width = 10, alpha = 0.2) +
  ylab("Compras (miles)") + xlab("Corte de tratamiento")
```



```{block2, type = "resumen"}
Los modelos que utilizamos generalmente son un insumo para tomar una decisión. 
Aunque usando técnicas estándar y medidas de error estándar podemos construir modelos
apropiados, no son suficientes para tomar esa decisión, cuyo contexto más amplio
de costo-beneficio debe ser considerado.
```


Una razón también a favor de usar medidas de error estándar es que 
- Como analistas o científicos de datos, muchas veces no tenemos completo el 
contexto de la decisión (especialmente en etapas tempranas de nuestro proyecto), 
y debemos proveer de guías o herramientas para tomar esa decisión en la que intervienen
más actores.
- En nuestro entrenamiento como científicos de datos nos concentramos en una cuantas
métricas que entendemos mejor, y que están o son fácilmente implementadas universalmente. Por tanto
es conveniente dejar para análisis posterior ad-hoc el análisis costo-beneficio particular
del problema que nos interesa.

*Ejemplo*: Los modelos que asignan números a casas en *street view* de google son modelos
estándar de procesamiento de imágenes. Sin embargo, la decisión de marcar o no en el mapa
un número requiere consideraciones especiales: por ejemplo, si existe algo de ambigüedad,
la decisión se inclina por no asignar números. El nivel de tolerancia se evalúa considerando
las consecuencias de etiquetar un lugar con un número equivocado.
