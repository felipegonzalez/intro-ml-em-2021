# Métodos locales: vecinos más cercanos

```{r, include = FALSE}
library(tidyverse)
theme_set(theme_minimal(base_size = 13))
cbb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = cbb_palette)
}
```

Una de las estrategias más simples para hacer predicciones es buscar en
nuestro conjunto de entrenamiento casos similares a los que queremos hacer
predicciones, y construir predicciones usando esos casos *similares*.

Por ejemplo, en el método de $k$ vecinos más cercanos, sea 
$\mathbf{x} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)$ el caso
para el que queremos hacer predicciones. Calculamos para nuestros casos
de entrenamiento $(x^{(i)}, y^{(i)})$ la distancia
$$d_i = dist(\mathbf{x}, x^{(i)})$$
y encontramos los $k$ casos de entrenamiento don $d_i$ más chica. Supongamos
que $N(\mathbf{x})$ representa el conjunto de estos $k$ vecinos más cercanos.
Nuestra predicción es entonces el promedio de las respuestas de estos $k$ vecinos:

$$\hat{f}(\mathbf{x}) = \frac{1}{k}\sum_{i\in N(\mathbf{x})} y^{(i)}.$$

La distancia puede ser seleccionada de distintas maneras: por ejemplo, es común
normalizar los datos para que tengan la misma escala, y utilizar la distancia euclideana.

También es posible utilizar una ponderación de casos, donde damos más peso a los valores cercanos y menos peso a los valores menos cercanos:

$$\hat{f}(\mathbf{x}) = \frac{\sum_{i\in N(\mathbf{x})} \phi_iy^{(i)}}{\sum_{i\in N(\mathbf{x})} \phi_i}$$

donde por ejemplo $\phi_i = \phi(||x^{(i)} - \mathbf{x}||).$ A la función $\phi$
le llamamos *kernel* para la distancia, y puede tener distintas formas, por ejemplo 

- Si $\phi$ es constante, se trata del kernel *rectangular*.
- Si usamos $\phi_i = \phi(d) = e^{-d^2/2}$, se trata del kernel *gaussiano*.

Revisa [esta liga](https://epub.ub.uni-muenchen.de/1769/1/paper_399.pdf) para
entender la implementación del paquete [@kknn] en R.

*Nota*: El *suavizamiento loess* que quizá has utilizado para producir suavizadores en gráficas
de *ggplot* es una variación de estos método de vecinos cercanos ponderados. Una diferencia
es que en este suavizamiento, en lugar de dar un número de vecinos cercanos a considerar,
usamos una *ventana* de distancia alrededor de cada punto:

```{r, fig.width=4, fig.height=3}
ggplot(mtcars, aes(x = disp, y = mpg)) +
  geom_point() +
  geom_smooth(method = "loess", span = 0.35, se = FALSE,
              method.args = list(degree = 0, family = "gaussian"))
# cuando usamos degree = 1 o degree = 2, el proceso de ponderación
# también incorpora una regresión local
```

## Controlando complejidad

En los métodos de vecinos más cercanos, podemos afinar flexibilidad-rigidez usando
distintos números de vecinos más cercanos:

- Cuando usamos 1 vecino más cercano, tenemos tantos parámetros como datos en la muestra
de entrenamiento (muy flexible).
- Si usamos $N$ vecinos más cercanos, sólo tenemos 1 parámetro por estimar, de forma
que sólo ajustamos una constante (muy rígido).
- Valores intermedios producen estimadores con distintos grados de suavidad.


```{r}
library(tidymodels)
set.seed(142)
simular_ejemplo <- function(n, sd = 500){
  x <- runif(n, 0, 20)
  y <- ifelse(x < 10, 1000*sqrt(x), 1000*sqrt(10))
  y <- y + rnorm(n, 0, sd = sd)
  tibble(id = 1:n, x = x, y = y)
}
datos_entrena <- simular_ejemplo(30)
datos_validacion <- simular_ejemplo(1000) %>% 
  mutate(id = 30 + id)
datos <- bind_rows(datos_entrena, datos_validacion)
ejemplo_split <- list()
# creamos un objeto con datos de entrenamiento y de prueba
ejemplo_split[[1]] <- make_splits(list("analysis" = 1:30, "assessment" = 31:1030), datos)
val_split <- manual_rset(ejemplo_split, "validación")
```

```{r}
# nótese que usamos tune() para el número de vecinos
modelo_vecinos <- nearest_neighbor(neighbors = tune(), weight_func = "gaussian") %>% 
  set_engine("kknn") %>% 
  set_mode("regression")
flujo_vecinos <- workflow() %>% add_model(modelo_vecinos) %>% add_formula(y~x)
vecinos_params <- parameters(neighbors(range = c(1, 25)))
vecinos_grid <- grid_regular(vecinos_params, levels = 25)
vecinos_grid
```
```{r}
vecinos_eval_tbl <- tune_grid(flujo_vecinos,
                            resamples = val_split,
                            grid = vecinos_grid,
                            metrics = metric_set(mae)) 
  
vecinos_ajustes_tbl <- vecinos_eval_tbl %>%
  unnest(cols = c(.metrics)) %>% 
  select(id, neighbors, .metric, .estimate)
vecinos_ajustes_tbl %>% DT::datatable()
```

Y ahora podemos examinar cómo se ve el error de validación para cada uno de los modelos:

```{r}
ggplot(vecinos_ajustes_tbl, aes(x = neighbors, y = .estimate)) + geom_point() + geom_line()
```

Y confirmamos que existe un balance apropiado entre sesgo y varianza alrededor de 3-15 vecinos
más cercanos. 

- ¿Qué pasaría si evaluáramos sobre el error de entrenamiento?
- Explica en qué modelos el sesgo es más problemático que la varianza y viceversa

Presentamos también la gráfica incluyendo el error de entrenamiento. Nótese que
la tendencia clara es un aumento en el error conforme el número de vecinos aumenta.
¿Por qué?

```{r}
ejemplo_split <- list()
# creamos un objeto con datos de entrenamiento y de prueba
ejemplo_split[[1]] <- make_splits(list("analysis" = 1:30, "assessment" = 1:30), datos_entrena)
val_split <- manual_rset(ejemplo_split, "entrenamiento")
vecinos_entrena_tbl <- tune_grid(flujo_vecinos,
                            resamples = val_split,
                            grid = vecinos_grid,
                            metrics = metric_set(mae)) %>% 
  unnest(cols = c(.metrics)) %>% 
  select(id, neighbors, .metric, .estimate)
validacion_tbl <- bind_rows(vecinos_ajustes_tbl, vecinos_entrena_tbl)
ggplot(validacion_tbl, aes(x = neighbors, y = .estimate, colour = id)) + geom_point() + geom_line()

```


Repetiremos nuestro ejercicio de simulación simple usando
un número más razonable de vecinos más cercanos, y un kernel que produzca
un suavizado más apropiado.

```{r}
mejor <- select_best(vecinos_eval_tbl, metric = "mae")
ajuste <- flujo_vecinos %>% 
  finalize_workflow(mejor) %>% 
  fit(data = datos_entrena)
```

```{r}
preds <- predict(ajuste, datos_validacion)
datos_eval <- preds %>% 
  bind_cols(datos_validacion)
eval_lineal <- mis_metricas(datos_eval, truth = y, estimate = .pred) %>% 
  mutate(modelo = "1-vecino") %>% select(modelo, everything())
eval_lineal
```

Nótese que mejoramos el desempeño predictivo. El ajuste se ve como sigue:

```{r, message = FALSE}
graf_x <- tibble(x = seq(0, 20, 0.1), id = NA)
preds_x <- predict(ajuste, graf_x)
datos_graf <- preds_x %>% bind_cols(graf_x)
ggplot(datos_entrena, aes(x = x)) +
  geom_line(data= datos_graf, aes(y = .pred), colour = "red") +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y), method = "lm", se = FALSE)
```


Y vemos que el desempeño es mejor que el modelo lineal y el de 1-vecinos más
cercanos. La forma de las predicciones parece mejor adaptada a los datos.


## Ejemplo: precios de casas

Resolveremos el ejemplo de predicción de precios de ventas de casas.

```{r, fig.width=8, fig.height=4, message=FALSE}
set.seed(68821)
source("R/casas_traducir_geo.R")
casas_split <- initial_split(casas, prop = 0.75)
casas_entrena <- training(casas_split)
```


```{r}
receta_casas <- 
  recipe(precio_miles ~ area_hab_m2 + 
           area_garage_m2 + 
           area_sotano_m2 +
           area_lote_m2 +
           calidad_gral + 
           aire_acondicionado +
           año_construccion +
           condicion_venta, 
         data = casas_entrena) %>% 
  step_filter(condicion_venta == "Normal") %>% 
  step_select(-condicion_venta, skip = TRUE) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(aire_acondicionado)
```

```{block2, type="observacion"}
- Obsérvese el paso de normalización en la receta. Esto es importante
pues consideramos variables en distintas escalas (por ejemplo, año de construcción,
tamaños de garage y de áreas habitables, etc.)
- Esta receta es necesaria aprenderla o entrenarla también (¿con qué media
se centran las variables por ejemplo?)
- Al incorporarla más adelante en un flujo que incluye el ajuste del modelo
podemos hacer el ajuste completo de preprocesamiento y modelo
```

Un error común, por ejemplo, es centrar con todos los datos disponibles
antes de centrar, o usar los datos de prueba para incorporar en la normalización.
Explica por qué esto viola los principios de la tarea predictiva fundamental.


Definimos el tipo de modelo que queremos ajustar

```{r}
casas_modelo <- nearest_neighbor(neighbors = 10, weight_func = "gaussian") %>% 
  set_engine("kknn")
```

```{r}
workflow_casas <- workflow() %>% 
  add_recipe(receta_casas) %>% 
  add_model(casas_modelo)
```

Ajustamos el flujo

```{r}
ajuste <- fit(workflow_casas, casas_entrena)
ajuste
```

Y ahora podemos hacer predicciones:

```{r}
set.seed(8)
casas_prueba <- testing(casas_split) 
ejemplos <- casas_prueba %>% sample_n(5)
predict(ajuste, ejemplos) %>% 
  bind_cols(ejemplos %>% select(precio_miles, area_hab_m2)) %>% 
  arrange(desc(precio_miles))
```

Y finalmente podemos evaluar nuestro modelo. En este caso usamos el
error promedio porcentual:

```{r}
metricas <- metric_set(mape, mae, rmse)
metricas(casas_prueba %>% bind_cols(predict(ajuste, casas_prueba)), 
     truth = precio_miles, estimate = .pred)
```

```{r, fig.width = 8, fig.height=5}
ggplot(casas_prueba %>% bind_cols(predict(ajuste, casas_prueba)),
       aes(x = .pred, y = precio_miles, colour = condicion_venta)) +
  geom_point() +
  geom_abline(colour = "red") +
  facet_wrap(~condicion_venta)
```

**Ejercicio**: prueba con distinto número de vecinos más cercanos como
hicimos en el ejemplo de datos simulados.


```{block2, type="resumen"}
- Los métodos locales son usualmente fácil de explicar e implementar (aunque hacer
  búsquedas de vecinos cercanos en una base grande puede no ser muy rápido).
- Los métodos locales parecen tener muy pocos supuestos, y parece que pueden adaptarse
a cualquier situación. Aparentan ser métodos "universales" en este sentido.
- Sin embargo, veremos por qué para problemas reales no funcionan muy bien: en problemas
reales tenemos más de unas cuantas variables, y en ese caso los métodos locales
pueden tener fallas graves.
```

## Ejemplo: métodos locales en dimensión alta

Consideramos el siguiente ejemplo de [@ESL] de un problema de predicción 
determinístico:

Consideremos que la salida Y es determinística $Y = e^{-8\sum_{j=1}^p x_j^2}$.
Vamos a usar 1-vecino más cercano para hacer predicciones, con 
una muestra de entrenamiento de 1000 casos.
Generamos $x^{i}$‘s uniformes en $[1,1]$, para $p = 2$, y calculamos la respuesta 
$Y$ para cada caso:

```{r}
fun_exp <- function(x) exp(-8 * sum(x ^ 2))
x <- map(1:1000, ~ runif(2, -1, 1))
dat <- tibble(x = x) %>% 
        mutate(y = map_dbl(x, fun_exp))
ggplot(dat %>% mutate(x_1 = map_dbl(x, 1), x_2 = map_dbl(x, 2)), 
       aes(x = x_1, y = x_2, colour = y)) + geom_point()
```

La mejor predicción en $x_0 = (0,0)$ es $f((0,0)) = 1$. El vecino más
cercano al origen es
```{r vmcbajadim}
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
mas_cercano; mas_cercano$x[[1]]
```
Nuestra predicción es entonces $\hat{f}(0)=$ `r mas_cercano$y`, que
es bastante cercano al valor verdadero (1).

Ahora intentamos hacer lo mismo para dimensión $p=8$.

```{r vmcalta}
x <- map(1:1000, ~ runif(8, -1, 1))
dat <- tibble(x = x) %>% 
       mutate(y = map_dbl(x, fun_exp))
dat <- dat %>% mutate(dist_origen = map_dbl(x, ~ sqrt(sum(.x^2)))) %>% 
  arrange(dist_origen)
mas_cercano <- dat[1, ]
# el vecino más cercano al origen es:
mas_cercano$x[[1]]
```

Y el resultado es catastrófico. Nuestra predicción puntual es
```{r}
mas_cercano$y
```

Necesitaríamos una muestra de alrededor de  un millón de casos
para obtener resultados no tan malos (pruébalo). Si la dimensión es más
alta que 8, lo cual no es ninguna situación excepcional en problemas
aplicados, entonces para obtener buen desempeño se requieren tamaños
de datos que no ocurren en la práctica.

¿Qué es lo que está pasando? La razón es que en dimensiones altas, 
los puntos de la muestra de entrenamiento están muy lejos 
unos de otros, y están cerca de la frontera, 
incluso para tamaños de muestra relativamente grandes como n = 1000.
Cuando la dimensión crece, la situación empeora exponencialmente.

```{block2, type="resumen"}
**La maldición de la dimensionalidad**

En dimensiones altas, todos los conjuntos de entrenamiento factibles
se distribuyen de manera rala en el espacio de entradas: los puntos de entrenamiento
típicamente están lejanos a prácticamente cualquier punto en el que queramos
hacer predicciones. El desempeño de los **métodos locales no estructurados**
es mediocre o malo usualmente.
```




